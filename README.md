# Musician Hand: Robot Learning to Play a 4-Note Melody from Scratch

This project, "Musician Hand," explores the exciting domain of robot learning through a perception-action loop. The core objective is to train a robotic system to autonomously play a simple 4-note melody, starting entirely from a "scratch" state using a 2-minute babbling phase. This endeavor demonstrates a full learning pipeline, from data acquisition through exploration to inverse model training and final melodic performance.

Project Overview

The "Musician Hand" project represents an end-to-end learning system designed to imbue a robot with a musical skill. Instead of explicitly programming the robot's movements for each note, we leverage principles of self-supervised learning:

1.  Exploratory Babbling: The robot performs random, diverse actions (limb activations) for a short period (2 minutes).
2.  Perception-Action Data Collection: During babbling, the system simultaneously records the robot's commanded actions and its sensory output (e.g., the sound produced by its movements).
3.  Inverse Model Learning: A neural network is trained to learn the inverse dynamics â€“ mapping desired musical outputs (e.g., specific note characteristics from a melody's spectrogram) back to the required robot limb activations that produced those sounds during babbling.
4.  Melody Playback (Deployment): Once the inverse model is trained, a target 4-note melody is fed into the model, which then predicts the necessary robot activations to reproduce the melody. These activations are sent to the robot in real-time.

This approach simulates how biological systems learn motor skills through exploration and sensory feedback, enabling the robot to "understand" how its actions translate into musical outcomes.

Core Concepts & Methodology

1. Perception-Action Loop

The foundation of "Musician Hand" is a continuous feedback loop:
* Action: The robot executes limb activations (outputs to motors).
* Perception: A sensor (e.g., microphone) captures the resulting sound/vibration.
* Learning: This input-output pair is used to train a model that effectively closes the loop, allowing the robot to predict actions for desired perceptions.

2. Babbling Phase (Data Collection)

* Duration: A critical, short 2-minute period.
* Process: The robot's four "limbs" (actuators) are commanded to perform a series of structured, yet semi-random, activation patterns. These patterns are designed to explore the robot's motor space.
* Data Generated: During this phase, two synchronized datasets are collected:
    * Robot Activations (Inputs): The precise activation values sent to the robot's limbs.
    * Audio Output (Perception): The sound generated by the robot's movements, recorded simultaneously. This audio serves as the "perception" data.

3. Inverse Mapping Learning

The central challenge is to learn the function f-1: Music Features -> Robot Activations. This is a supervised learning problem where:
* Inputs: Features extracted from the audio (spectrograms) recorded during the babbling phase.
* Targets: The corresponding robot limb activations that produced those audio features.

Two distinct neural network architectures are employed for this mapping, tailored to different aspects of the musical input:

* music_amp_neural_net (Feed-forward Network):
    * Purpose: Primarily designed for learning mappings where global musical amplitude characteristics or simpler patterns in the spectrogram are most relevant.
    * Architecture: A multi-layer perceptron (MLP) that flattens the spectrogram input before passing it through dense (fully connected) layers.
    * Output: Generates 4 outputs, typically representing categorical decisions (e.g., which of 4 notes is active) or scaled amplitude levels if 'softmax' is replaced by 'linear' for regression.

* music_notes_neural_net (Convolutional Neural Network - CNN):
    * Purpose: Ideal for extracting spatial features from the spectrogram, such as distinct frequency patterns corresponding to specific notes or harmonies.
    * Architecture: Utilizes 2D convolutional layers and max-pooling layers to automatically learn hierarchical features from the spectrogram image-like input, followed by dense layers.
    * Output: Also generates 4 outputs, making it suitable for classifying which of the 4 notes should be played.

4. Melody Playback (Deployment)

Once the inverse mapping model is trained, it can be used to generate the necessary robot commands for any 4-note melody.
* The target 4-note melody's spectrogram (or relevant features) is fed into the trained inverse model.
* The model outputs the predicted limb activations.
* These activations are then streamed to the physical or simulated robot via a real-time communication bridge, causing the robot to "play" the melody.

Project Components

* functions.py:
    * Contains utility functions for the entire project.
    * systemID_input_gen_func: Generates structured, exploratory activation signals for the babbling phase.
    * compute_stft: Computes Short-Time Fourier Transform (STFT) for audio feature extraction (spectrograms).
    * note_to_freq: Converts MIDI note numbers to frequencies.
    * create_simple_nn: A general helper for creating basic Keras Sequential models.
    * generate_and_save_midi_audio: Synthesizes audio from MIDI input (live or simulated) and saves it, useful for testing audio perception.
    * music_amp_neural_net: Implements the feed-forward neural network for inverse mapping.
    * music_notes_neural_net: Implements the Convolutional Neural Network (CNN) for inverse mapping.
* Babbling_code.py:
    * Orchestrates the 2-minute babbling phase.
    * Generates the limb activation sequences using systemID_input_gen_func.
    * (Requires RTBridge) Sends these activations to the robot and (ideally) simultaneously collects the robot's audio output (this collection logic needs to be integrated with RTBridge or a similar framework).
    * Saves the generated activation data for later training.
* Train.py:
    * Loads the collected babbling data (activations and corresponding music features).
    * (Integration Point) Selects and trains either music_amp_neural_net or music_notes_neural_net (or a more complex custom model) as the inverse mapping.
    * Uses the trained model to predict robot activations for a target melody (from a source music spectrogram).
    * (Requires RTBridge) Deploys these predicted activations to the robot in real-time for melody playback.
* datalog/: A directory for storing recorded babbling activations, generated music spectrograms, trained model weights, and any other experiment-specific data.

Setup and Running the Project

Prerequisites

* Python 3.x
* Core Python libraries: numpy, scipy, tensorflow (which includes keras), matplotlib, scikit-learn, pyaudio, mido.

Installation

1.  Clone the repository:
    ```
    git clone https://github.com/hesamazj/MusicianHand.git
    cd Musician-Hand
    ```
2.  Install dependencies:
    ```
    pip install numpy scipy tensorflow matplotlib scikit-learn pyaudio mido
    ```
    * Note on mido and MIDI Drivers: For mido to work with live MIDI input, you may need to install platform-specific MIDI drivers (e.g., portmidi or rtmidi). Consult mido's documentation for your OS.
    * Note on RTBridge: The RTBridge library is highly specific to a particular real-time hardware setup and is not publicly available. The code segments related to RTBridge are commented out by default. If you intend to run this with a physical robot, you will need to:
        * Obtain and install the RTBridge library specific to your robot.
        * Uncomment the RTBridge sections in Babbling_code.py and Train.py.
        * Configure the pxiWin10 and pubPort variables to match your hardware's network settings.
        * Without RTBridge, Babbling_code.py will still generate activation files, and Train.py can train the inverse model, but real-time robot interaction will not occur.

Running the Workflow

The project follows a sequential workflow:

1.  Babbling (Data Collection):
    Execute Babbling_code.py to generate the exploratory robot limb activations and (conceptually) collect corresponding audio.
    ```
    python Babbling_code.py
    ```
    This will save activations.txt in a datalog/Experiment_v2_01 (or similar) directory. You would then need to manually extract audio features (spectrograms) from the collected audio data, aligning them with these activations, to prepare the dataset for training.

2.  Training the Inverse Mapping Model:
    Execute Train.py to train the neural network that maps music features to robot activations.
    * Before running Train.py, ensure you have:
        * activations.txt from the babbling phase (in datalog/Experiment_v2_01).
        * Corresponding music_spectrogram_raw data (the raw audio features from babbling), pre-processed and saved (this step might require additional scripts not explicitly provided here, assuming it's part of your spect_preprocessing pipeline). The current Train.py has dummy data for music_spectrogram_raw.
        * You'll need to decide which inverse mapping model (music_amp_neural_net or music_notes_neural_net) to integrate into Train.py based on your specific musical feature and output mapping strategy. The Train.py script provided uses create_simple_nn for the inverse model training.
    ```
    python Train.py
    ```
    This script will train a model and save predicted_activations.txt (which are the model's output given the input music spectrogram).

3.  Melody Playback (Deployment - with RTBridge):
    * The latter part of Train.py contains the deployment logic, which sends the predicted_activations to the robot via RTBridge.
    * To run this, you must have RTBridge configured and connected to your robot.

Expected Outcomes

Upon successful execution, the project aims to demonstrate:
* The ability to collect structured motor babbling data.
* A trained neural network model capable of inverting the robot's dynamics, mapping desired musical characteristics (from a spectrogram) to precise robot limb commands.
* A robot that, when fed a novel 4-note melody, can articulate the corresponding sounds by moving its limbs according to the model's predictions, thus "playing" the melody.

Potential Future Work

* Real-time Audio Processing: Integrate a real-time audio input stream during babbling for more direct perception.
* Reinforcement Learning for Exploration: Implement an RL agent to learn more efficient babbling strategies, rather than purely random/pre-defined patterns.
* Adaptive Control: Explore adaptive control techniques to fine-tune robot movements based on immediate auditory feedback.
* Expanded Musical Repertoire: Extend the system to handle more complex melodies, rhythms, or instruments.
* Physical Hardware Integration: Detailed documentation and examples for setting up the RTBridge with specific robot platforms.
